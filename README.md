# LLaDA

Rozpocznę przygotowywanie szczegółowego opracowania na temat rozwoju dyfuzyjnych modeli językowych, uwzględniając analizę porównawczą z tradycyjnymi podejściami autoregresywnymi. Dokument będzie obejmować matematyczne wzory, tabele porównawcze oraz szczegółowy opis metodologii i zastosowań. Po zakończeniu pracy udostępnię Ci gotowy materiał w formacie PDF zgodnym z arXiv.

# Rozwój dyfuzyjnych modeli językowych – porównanie podejścia autoregresywnego i dyfuzyjnego

## 1. Wprowadzenie

**Autoregresyjne modele językowe (AR)** stanowią od lat podstawę generatywnych modeli NLP. Przykładem są architektury Transformer takie jak GPT-3 czy GPT-4, które generują tekst **sekwencyjnie token po tokenie** – każdy kolejny wyraz jest przewidywany na podstawie wszystkich poprzednich ([Silicon Valley Startup Inception Labs Creates Faster LLM](https://www.pymnts.com/artificial-intelligence-2/2025/silicon-valley-startup-inception-labs-creates-faster-llm/#:~:text=%E2%80%9CAll%20existing%20large%20language%20models,%E2%80%9D)). Formalnie, modele autoregresyjne modelują rozkład zdań poprzez łańcuch zależności warunkowych: 

$$
P(x_1, x_2, \dots, x_n) \;=\; \prod_{i=1}^{n} P(x_i \mid x_1, x_2, \dots, x_{i-1})\,,
$$

gdzie $x_i$ to kolejne tokeny (np. słowa) w sekwencji. Trenowanie odbywa się metodą maksymalizacji wiarygodności (MLE), minimalizując sumaryczną entropię krzyżową $-\sum_i \log P_\theta(x_i \mid x_{<i})$. Takie podejście okazało się niezwykle skuteczne – pozwoliło wyszkolić modele językowe o niespotykanej płynności wypowiedzi i zdolnościach wnioskowania. **Transformery autoregresyjne** wyparły wcześniejsze RNN i LSTM dzięki możliwości efektywnego uczenia na ogromnych korpusach tekstu, co przełożyło się na imponujące wyniki w zadaniach od tłumaczenia po chat-boty.

Mimo sukcesów, klasyczne modele autoregresyjne posiadają istotne **ograniczenia** wynikające z ich sekwencyjnej natury. Po pierwsze, **generacja tekstu jest szeregowa**, co **uniemożliwia pełną równoległość** – model musi wygenerować każdy token po kolei. Dla długich sekwencji powoduje to duże opóźnienia i wysokie koszty obliczeniowe, ponieważ nie można przewidzieć następnego słowa zanim nie wygeneruje się poprzedniego ([Silicon Valley Startup Inception Labs Creates Faster LLM](https://www.pymnts.com/artificial-intelligence-2/2025/silicon-valley-startup-inception-labs-creates-faster-llm/#:~:text=%E2%80%9CAll%20existing%20large%20language%20models,%E2%80%9D)) ([What are Large Language Diffusion with mAsking (LLaDA)? Redefining Language Generation with Diffusion Models](https://learnprompting.org/blog/large-language-diffusion-models?srsltid=AfmBOoomRpUimmugFjLV0v1gQU6cEHp2e7rSpxHKHBpoCnCzmXs2eO5x#:~:text=,time%20responses)). Po drugie, **model ma ograniczony kontekst kierunkowy** – w momencie przewidywania kolejnego tokenu widzi on tylko **przeszłe** słowa, a nie zna przyszłych ([What are Large Language Diffusion with mAsking (LLaDA)? Redefining Language Generation with Diffusion Models](https://learnprompting.org/blog/large-language-diffusion-models?srsltid=AfmBOoomRpUimmugFjLV0v1gQU6cEHp2e7rSpxHKHBpoCnCzmXs2eO5x#:~:text=,time)). Brak dostępu do **kontekstu dwukierunkowego** (zarówno poprzedzającego, jak i następującego) może utrudniać modelowi uchwycenie globalnej struktury zdania lub dokumentu. W praktyce skutkuje to problemami z **długoterminową spójnością** generowanego tekstu oraz trudnością w zadaniach, gdzie przydałaby się wiedza o **docelowym zakończeniu zdania lub akapitu**. 

Kolejnym wyzwaniem jest tzw. **eksplozja błędów (exposure bias)** – podczas treningu model widzi zawsze poprawne prefiksy zdań, a podczas generacji musi radzić sobie z własnymi (czasem błędnymi) wyjściami jako kontekstem. Może to prowadzić do **halucynacji** lub zapętlania się modelu. Autoregresyjne LLM-y mają także ograniczone możliwości **korekcji błędów w trakcie generacji** – jeśli model wygeneruje niepasujący fragment, nie potrafi go później poprawić bez rozpoczęcia procesu od nowa.

W związku z powyższymi ograniczeniami, w ostatnim czasie rośnie zainteresowanie **alternatywnymi paradygmatami generowania tekstu**, które mogłyby przełamać sekwencyjność i umożliwić bardziej efektywne i elastyczne generowanie. Jednym z najbardziej obiecujących kierunków jest zastosowanie **modeli dyfuzyjnych** (ang. *diffusion models*) do zadań językowych. Modele te, które zrewolucjonizowały generowanie obrazów, audio i wideo, stanowią nowatorskie podejście również dla języka naturalnego. W dalszych sekcjach omówimy zasadę działania modeli dyfuzyjnych, porównamy szczegółowo podejście autoregresywne i dyfuzyjne (zarówno pod względem teoretycznym, jak i praktycznym), a także przeanalizujemy najnowsze osiągnięcia literaturowe – w szczególności prace nad rodziną modeli **Mercury** ([Silicon Valley Startup Inception Labs Creates Faster LLM](https://www.pymnts.com/artificial-intelligence-2/2025/silicon-valley-startup-inception-labs-creates-faster-llm/#:~:text=%E2%80%9CThe%20key%20advantage%20is%20that,words%2C%20in%20parallel%2C%E2%80%9D%20he%20said)), modelem **LLaDA** ([Paper Review: Large Language Diffusion Models – Andrey Lukyanenko](https://andlukyane.com/blog/paper-review-llada#:~:text=LLaDA%20demonstrates%20strong%20scalability%2C%20outperforming,poem%20completion%20task%2C%20addressing%20the)) oraz metodą **Block Diffusion** ([Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models](https://chatpaper.com/chatpaper/paper/120009#:~:text=Block%20diffusion%20sequentially%20generates%20blocks,KV%20caching%20and%20parallel%20sampling)). Na koniec przedstawimy wnioski oraz perspektywy rozwoju dyfuzyjnych modeli językowych.

## 2. Modele dyfuzyjne w generowaniu języka

**Modele dyfuzyjne** to generatywne modele, które działają poprzez **iteracyjne ulepszanie początkowo losowego (lub zaszumionego) sygnału**. W przypadku obrazów oznacza to zaczynanie od obrazku wypełnionego szumem i stopniowe przekształcanie go w fotorealistyczny obraz poprzez serię denoisingowych kroków. Sukces takich metod w domenie wizualnej (np. algorytmy pokroju *DALL·E 2* czy *Stable Diffusion*) zainspirował badaczy do zastosowania analogicznej idei w generowaniu tekstu ([Silicon Valley Startup Inception Labs Creates Faster LLM](https://www.pymnts.com/artificial-intelligence-2/2025/silicon-valley-startup-inception-labs-creates-faster-llm/#:~:text=Inception%20Labs%20applies%20a%20technique,computer%20science%20at%20Stanford%20University)). 

W kontekście językowym, model dyfuzyjny **generuje cały tekst w wielu etapach**, a nie słowo po słowie. Zaczyna od pewnej **szkicowej odpowiedzi** (np. losowego ciągu tokenów lub specjalnie oznaczonych pustych miejsc) i następnie **wielokrotnie poprawia** tę odpowiedź, aż uzyska docelowy, spójny ciąg znaków ([Silicon Valley Startup Inception Labs Creates Faster LLM](https://www.pymnts.com/artificial-intelligence-2/2025/silicon-valley-startup-inception-labs-creates-faster-llm/#:~:text=Inception%20Labs%20applies%20a%20technique,computer%20science%20at%20Stanford%20University)). Intuicyjnie możemy to porównać do pisania zdania ołówkiem i wielokrotnego wymazywania oraz poprawiania słów, dopóki zdanie nie nabierze poprawnej formy. W odróżnieniu od modelu autoregresywnego, który jest *“ślepy na przyszłość”* i w każdym kroku patrzy tylko na przeszłe tokeny, model dyfuzyjny na każdym etapie widzi **cały kontekst zdania (zarówno to, co już wypełnione, jak i luki)**. Dzięki temu może wypełniać luki uwzględniając informacje **z obu stron** danego miejsca ([What are Large Language Diffusion with mAsking (LLaDA)? Redefining Language Generation with Diffusion Models](https://learnprompting.org/blog/large-language-diffusion-models?srsltid=AfmBOoomRpUimmugFjLV0v1gQU6cEHp2e7rSpxHKHBpoCnCzmXs2eO5x#:~:text=1,part%20of%20the%20training%20data)) ([What are Large Language Diffusion with mAsking (LLaDA)? Redefining Language Generation with Diffusion Models](https://learnprompting.org/blog/large-language-diffusion-models?srsltid=AfmBOoomRpUimmugFjLV0v1gQU6cEHp2e7rSpxHKHBpoCnCzmXs2eO5x#:~:text=The%20combination%20of%20a%20randomized,both%20past%20and%20future%20context)). 

Kluczowym elementem jest zdefiniowanie procesu **niszczenia danych (forward process)** oraz procesu **odbudowy (reverse process)**. W przypadku języka zamiast dodawania ciągłego szumu Gaussowskiego (jak w obrazach), typowym podejściem jest losowe **maskowanie** części tokenów. **Proces do przodu** polega więc na stopniowym zastępowaniu oryginalnego tekstu lukami (maskami) – np. z każdą kolejną “iteracją czasu” rośnie odsetek zamaskowanych słów, aż do uzyskania całkowicie zamazanego ciągu (wszystkie tokeny to symbol maskujący). Formalnie możemy zdefiniować nieciągły proces Markowa $q(x_t \mid x_{t-1})$, który niezależnie dla każdego tokenu z $x_{t-1}$ zamienia go na symbol `<MASK>` z pewnym prawdopodobieństwem $\beta_t$. Dla $t=T$ otrzymujemy $x_T$ będący całkowicie zamaskowaną sekwencją (odpowiednik maksymalnego szumu). **Proces odtworzenia (denoising)** polega natomiast na odwróceniu tego procesu: model uczy się przewidywać oryginalne tokeny na podstawie częściowo zamaskowanego kontekstu. Startując od $x_T$ (wszystko zamaskowane), model kolejno **odmaskowuje** fragmenty tekstu. Na każdym kroku $t \to t-1$ sieć (np. transformator) obserwuje bieżący ciąg $x_t$ i **przewiduje brakujące tokeny** (zamaskowane pozycje) w $x_{t-1}$ ([Paper Review: Large Language Diffusion Models – Andrey Lukyanenko](https://andlukyane.com/blog/paper-review-llada#:~:text=LLaDA%20models%20distributions%20using%20a,applied%20only%20to%20masked%20tokens)). Proces ten iteruje, aż dotrzemy do $x_0$, które powinno być w pełni zrekonstruowanym tekstem. Warto zauważyć, że w **każdym kroku model uzupełnia wiele brakujących tokenów jednocześnie**, działając **równolegle na wielu pozycjach** sekwencji ([Silicon Valley Startup Inception Labs Creates Faster LLM](https://www.pymnts.com/artificial-intelligence-2/2025/silicon-valley-startup-inception-labs-creates-faster-llm/#:~:text=%E2%80%9CThe%20key%20advantage%20is%20that,words%2C%20in%20parallel%2C%E2%80%9D%20he%20said)) – to fundamentalna różnica względem podejścia autoregresywnego.

Trenowanie modelu dyfuzyjnego odbywa się poprzez maksymalizację tzw. **dolnej granicy log-wiarygodności (ELBO)** danych treningowych. W praktyce sprowadza się to do minimalizacji oczekiwanego błędu odtworzenia na każdym etapie denoisingu. Jeśli oznaczymy przez $x^0$ oryginalną sekwencję (dane), a $x^T$ – całkowicie zamaskowaną sekwencję (szum), to trening minimalizuje sumę strat na poszczególnych krokach:

<img src="https://render.githubusercontent.com/render/math?math=\mathcal{L}_{\text{diff}} \;=\; \mathbb{E}_{x^0}\; \sum_{t=1}^{T} \mathbb{E}_{x^t \sim q(x^t \mid x^0)} \Big[-\log P_\theta(x^{\,t-1} \mid x^t)\Big]\,,">

gdzie $P_\theta(x^{t-1} \mid x^t)$ modeluje rozkład tokenów przed denoisingiem ($x^{t-1}$) na podstawie ich zaszumionej wersji ($x^t$). W praktyce implementacyjnej często upraszcza się to poprzez *trening z losowym krokiem* – każda próbka treningowa przyjmuje losowy poziom maskowania $t \sim \text{Unif}(0,1)$ i model uczy się rekonstruować oryginał z tak powstałej maskowanej sekwencji ([What are Large Language Diffusion with mAsking (LLaDA)? Redefining Language Generation with Diffusion Models](https://learnprompting.org/blog/large-language-diffusion-models?srsltid=AfmBOoomRpUimmugFjLV0v1gQU6cEHp2e7rSpxHKHBpoCnCzmXs2eO5x#:~:text=1,part%20of%20the%20training%20data)). Dzięki losowaniu różnych maskowań model nabywa umiejętność rekonstrukcji tekstu z dowolnego stopnia “zniszczenia”. 

**Różnice względem klasycznego maskowanego modelu językowego (MLM)**: Warto podkreślić, że choć trening dyfuzyjnego LLM przypomina maskowane modele językowe pokroju BERT (gdzie pewien procent tokenów jest maskowany i przewidywany), to są kluczowe różnice. Po pierwsze, **maskowanie odbywa się z losowym, zmiennym udziałem tokenów (od 0% do 100%)**, zamiast stałego odsetka ([Paper Review: Large Language Diffusion Models – Andrey Lukyanenko](https://andlukyane.com/blog/paper-review-llada#:~:text=The%20training%20objective%20is%20an,scale%20applications)). To oznacza, że model bywa trenowany czasem na bardzo łatwych przykładach (prawie cały oryginalny tekst widoczny), a czasem na ekstremalnie trudnych (prawie cały tekst zasłonięty). Taka **różnorodność wzorców maskowania** poprawia ogólną skuteczność i **zdolność skalowania** modelu ([Paper Review: Large Language Diffusion Models – Andrey Lukyanenko](https://andlukyane.com/blog/paper-review-llada#:~:text=The%20training%20objective%20is%20an,scale%20applications)). Po drugie, model nie jest używany jednorazowo, lecz **wielokrotnie iteracyjnie** podczas generacji – trenowany jest tak, by **poprawiać swoje własne przewidywania** w kolejnych krokach (poprzez mechanizm *remaskowania*, o którym niżej). To odróżnia go od BERT, który po jednorazowym uzupełnieniu masek nie może dalej ulepszać wyniku. 

**Kontrolowane odmaskowywanie (remasking):** W praktyce generacji dyfuzyjnej często stosuje się strategię ponownego maskowania pewnych tokenów po każdym kroku, aby stopniowo poprawiać tylko te najbardziej niepewne fragmenty. Przykładowo, model LLaDA podczas próbkowania identyfikuje najniżej ocenione (najmniej pewne) przewidziane tokeny i **ponownie je maskuje** przed następnym krokiem, aby mieć kolejną szansę na ich poprawę ([Paper Review: Large Language Diffusion Models – Andrey Lukyanenko](https://andlukyane.com/blog/paper-review-llada#:~:text=number%20of%20sampling%20steps%20controls,off%20between%20efficiency%20and%20quality)). Taka *elastyczna re-maskowanie* powoduje, że nawet jeśli model pomylił się co do jakiegoś słowa w jednym kroku, może to słowo skorygować w kolejnych iteracjach – co jest niemożliwe w podejściu AR, gdzie raz wygenerowany token pozostaje już w tekście na stałe. Iteracyjne poprawianie ogranicza zjawisko **halucynacji i niespójności**, ponieważ model ma okazję naprawić sprzeczności zanim zakończy generowanie ([What are Large Language Diffusion with mAsking (LLaDA)? Redefining Language Generation with Diffusion Models](https://learnprompting.org/blog/large-language-diffusion-models?srsltid=AfmBOoomRpUimmugFjLV0v1gQU6cEHp2e7rSpxHKHBpoCnCzmXs2eO5x#:~:text=,hallucinations%20and%20improving%20overall%20accuracy)) ([What are Large Language Diffusion with mAsking (LLaDA)? Redefining Language Generation with Diffusion Models](https://learnprompting.org/blog/large-language-diffusion-models?srsltid=AfmBOoomRpUimmugFjLV0v1gQU6cEHp2e7rSpxHKHBpoCnCzmXs2eO5x#:~:text=,from%20detailed%20dialogue%20systems%20to)).

**Złożoność i wymagania obliczeniowe:** Modele dyfuzyjne wymagają wykonania kilku lub kilkunastu iteracji sieci neuronowej, aby wygenerować finalny tekst. Na pierwszy rzut oka może się wydawać, że to więcej pracy niż pojedynczy przebieg modelu autoregresywnego generującego słowo po słowie. Jednak kluczową różnicą jest wspomniana **równoległość**: w pojedynczym kroku dyfuzyjnym model przetwarza *całą sekwencję naraz*. Jeśli liczba kroków dyfuzji $T$ jest dużo mniejsza niż długość sekwencji $n$, wówczas generacja dyfuzyjna może być szybsza od autoregresywnej. Przykładowo, jeśli chcemy wygenerować 100 tokenów, model AR wykona 100 kroków (jeden dla każdego tokenu), podczas gdy model dyfuzyjny mógłby osiągnąć porównywalną jakość w, powiedzmy, 10 iteracjach refinowania całej sekwencji. W efekcie dyfuzja może zaoferować **niższy czas odpowiedzi** oraz **niższy koszt obliczeń** dzięki mniejszej liczbie kroków sieci, jak sugerują pierwsze komercyjne implementacje ([Silicon Valley Startup Inception Labs Creates Faster LLM](https://www.pymnts.com/artificial-intelligence-2/2025/silicon-valley-startup-inception-labs-creates-faster-llm/#:~:text=Diffusion%20is%20faster%2C%20he%20said%2C,hours%20needed%20for%20using%20GPUs)). Oczywiście, odpowiednie dobranie liczby kroków jest kompromisem między szybkością a jakością – więcej iteracji pozwala uzyskać lepszy tekst kosztem czasu, podczas gdy zbyt mało iteracji może skutkować niedopracowaną wypowiedzią.

Podsumowując, modele dyfuzyjne przenoszą ideę iteracyjnego generowania ze świata obrazów do świata języka. Ich najważniejsze cechy to: **generacja globalna** (całej sekwencji) zamiast lokalnej, **wykorzystanie kontekstu pełnego zdania** na każdym etapie, możliwość **równoległego wypełniania wielu tokenów** oraz **iteracyjna korekta błędów**. Te właściwości obiecują rozwiązać niektóre bolączki klasycznych LLM, jak wolne generowanie czy brak globalnej koherencji. Zanim jednak stwierdzimy, czy modele dyfuzyjne faktycznie przewyższają autoregresywne, przeanalizujmy szczegółowo różnice obu podejść.

 ([Paper Review: Large Language Diffusion Models – Andrey Lukyanenko](https://andlukyane.com/blog/paper-review-llada)) *Figura 1: Koncepcyjny schemat działania modelu LLaDA (Large Language Diffusion with mAsking) ([Paper Review: Large Language Diffusion Models – Andrey Lukyanenko](https://andlukyane.com/blog/paper-review-llada#:~:text=LLaDA%20models%20distributions%20using%20a,applied%20only%20to%20masked%20tokens)). (a) *Pre-training*: losowe maskowanie wszystkich tokenów z odsetkiem $t \sim U[0,1]$. (b) *Fine-tuning*: maskowane są tylko tokeny odpowiedzi (prompt pozostaje jawny). (c) *Sampling*: generowanie poprzez symulację procesu dyfuzji od $t=1$ (wszystko zamaskowane) do $t=0$ (pełna rekonstrukcja). Model przewiduje jednocześnie wszystkie zamaskowane tokeny, a po każdym kroku może ponownie zamaskować wybrane (np. niepewne) pozycje (*remask*) przed kolejnym krokiem denoisingu. ([Paper Review: Large Language Diffusion Models – Andrey Lukyanenko](https://andlukyane.com/blog/paper-review-llada#:~:text=LLaDA%20models%20distributions%20using%20a,applied%20only%20to%20masked%20tokens)) ([Paper Review: Large Language Diffusion Models – Andrey Lukyanenko](https://andlukyane.com/blog/paper-review-llada#:~:text=number%20of%20sampling%20steps%20controls,off%20between%20efficiency%20and%20quality))*

## 3. Analiza porównawcza podejść autoregresywnego i dyfuzyjnego

W tej części porównujemy oba paradygmaty generowania języka pod kilkoma kluczowymi względami: formalnego modelowania rozkładów (wzory matematyczne), efektywności obliczeniowej, zastosowań praktycznych oraz zalet i wad.

### 3.1 Modelowanie probabilistyczne i wzory matematyczne

**Autoregresja:** Model autoregresywny jawnie modeluje rozkład prawdopodobieństwa ciągu tekstowego poprzez rozbicie go na iloczyn warunkowych, zgodnie z formułą łańcucha (podanej w sekcji 1). Dzięki temu trening sprowadza się do prostego przewidywania kolejnych tokenów – jest to zasadniczo **uczenie nadzorowane** polegające na minimalizacji entropii krzyżowej przewidywań modelu względem następnego słowa. Zaletą takiej faktoracji jest to, że dokładnie optymalizujemy docelową miarę jakości (log-wiarygodność danych). Wadą – że wynikowa generacja jest związana z ustaloną kolejnością (np. lewostronnym kontekstem).

**Dyfuzja:** Model dyfuzyjny definiuje generowanie jako **łańcuch ukrytych stanów (latentów)** $x_T \to x_{T-1} \to \cdots \to x_0$, gdzie $x_0$ to docelowy tekst, a $x_T$ – maksymalnie zaszumiona wersja (np. całkowicie zamaskowany tekst). Rozkład modelowany jest pośrednio: $P_\theta(x_0) = \sum_{x_1,\dots,x_T} P(x_T)\prod_{t=1}^T P_\theta(x_{t-1} \mid x_t)$. Przy założeniu, że proces do przodu $q(x_t \mid x_{t-1})$ jest znany (np. maskowanie z zadanym prawdopodobieństwem), można wyprowadzić dolną granicę log-wiarygodności i trenować model $\theta$ tak, by aproksymował odwrócenie tego procesu (patrz wyżej). W praktyce otrzymujemy **zbliżoną do BERT** funkcję kosztu: model uczy się odgadywać ukryte tokeny na podstawie widocznych, a **trudność tego zadania regulowana jest parametrem $t$** (im większe $t$, tym więcej brakujących elementów). Można to postrzegać jako **uczenie modelu do uzupełniania luk dla każdego możliwego scenariusza zniszczenia danych**. Dzięki temu po treningu model potrafi generować pełne zdania iteracyjnie. Choć trening jest bardziej złożony (bo optymalizujemy wieloetapowy proces, a nie bezpośrednio log-likelihood jak w AR), to wykazano, że tak wyszkolony model dyfuzyjny zachowuje własność **zgodności Fishera** (Fisher consistency) – w granicy doskonałego dopasowania zapewnia to poprawne modelowanie docelowego rozkładu ([Paper Review: Large Language Diffusion Models – Andrey Lukyanenko](https://andlukyane.com/blog/paper-review-llada#:~:text=The%20training%20objective%20is%20an,scale%20applications)).

**Porównanie**: W podejściu AR cała niepewność modelu koncentruje się na **następnym tokenie**, natomiast w dyfuzji jest ona rozłożona na wiele kroków refinowania całej sekwencji. AR minimalizuje bezpośrednio błąd predykcji kolejnych słów, podczas gdy dyfuzja minimalizuje błąd rekonstrukcji na kolejnych poziomach zaszumienia. Oba podejścia dążą do modelowania tego samego rozkładu języka, lecz czynią to w inny sposób: AR poprzez **jawną faktorację sekwencji w czasie**, dyfuzja poprzez **faktorację iteracyjną procesu denoisingu**. Można zauważyć pewną analogię: w AR indeks $i$ (pozycja tokenu) pełni rolę “czasu” generacji, zaś w modelu dyfuzyjnym rolę czasu pełni indeks kroku $t$ (poziom szumu). W AR zwiększanie $i$ odkrywa nowy fragment sekwencji, w dyfuzji zmniejszanie $t$ odkrywa nowe fragmenty (poprzez usuwanie masek). Matematycznie, AR zakłada *łańcuch tokenów*, a dyfuzja *łańcuch stanów pośrednich*. 

### 3.2 Efektywność obliczeniowa i wymagania

Jedną z głównych motywacji wprowadzania modeli dyfuzyjnych dla języka jest potencjalna poprawa **efektywności generacji**. W modelu AR generowanie $n$-tokenowej sekwencji wymaga $n$ kroków sieci (każdy dostarcza kolejny token). Co gorsza, kroki te **nie mogą być zrównoleglone** na jednym ciągu – każdy zależy od wyniku poprzedniego. Oznacza to, że **czas odpowiedzi rośnie liniowo** wraz z długością żądanego tekstu. Istnieje co prawda możliwość równoległego generowania wielu sekwencji jednocześnie (przez batchowanie), ale dla pojedynczego zapytania nie da się tego przyspieszyć. Dodatkowo, choć mechanizm **keszowania kluczy i wartości (KV caching)** w Transformerze pozwala uniknąć powtarzania obliczeń dla już wygenerowanych tokenów, to i tak każdy nowy token wymaga pełnego przeliczenia wszystkich warstw atencji (self-attention) z rosnącym kontekstem.

W modelach dyfuzyjnych sekwencja długości $n$ jest generowana w $T$ iteracjach, gdzie $T$ jest **z góry ustalonym parametrem** (niezależnym od $n$). Co więcej, każdy z tych $T$ kroków to **transformacja całej sekwencji jednocześnie**, co doskonale nadaje się do równoległego wykonania na nowoczesnych akceleratorach (GPU/TPU). W efekcie, jeśli $T \ll n$, możemy uzyskać znaczący **wzrost szybkości generowania**. Przykładowo, startup Inception Labs ogłosił, że ich dyfuzyjny model **Mercury** osiąga przepustowość aż **1000 tokenów na sekundę** na karcie Nvidia H100 ([Silicon Valley Startup Inception Labs Creates Faster LLM](https://www.pymnts.com/artificial-intelligence-2/2025/silicon-valley-startup-inception-labs-creates-faster-llm/#:~:text=Inception%20Labs%20hit%20a%20record,Flash)) – co oznacza przyspieszenie rzędu 5–10x względem typowych modeli autoregresywnych o podobnych możliwościach ([Silicon Valley Startup Inception Labs Creates Faster LLM](https://www.pymnts.com/artificial-intelligence-2/2025/silicon-valley-startup-inception-labs-creates-faster-llm/#:~:text=A%20Silicon%20Valley%20startup%20founded,higher%20quality%20than%20existing%20models)) ([Silicon Valley Startup Inception Labs Creates Faster LLM](https://www.pymnts.com/artificial-intelligence-2/2025/silicon-valley-startup-inception-labs-creates-faster-llm/#:~:text=Diffusion%20is%20faster%2C%20he%20said%2C,hours%20needed%20for%20using%20GPUs)). Tak duża różnica wynika właśnie z możliwości **przetwarzania wielu tokenów równocześnie** zamiast jeden po drugim. Warto zauważyć, że modele dyfuzyjne mogą być też **tańsze w uruchamianiu na dużą skalę**, ponieważ skrócenie czasu generacji zmniejsza łączny czas wykorzystania GPU potrzebny do obsługi zapytań ([Silicon Valley Startup Inception Labs Creates Faster LLM](https://www.pymnts.com/artificial-intelligence-2/2025/silicon-valley-startup-inception-labs-creates-faster-llm/#:~:text=Diffusion%20is%20faster%2C%20he%20said%2C,hours%20needed%20for%20using%20GPUs)).

Z drugiej strony, modele dyfuzyjne często wymagają więcej pamięci i mocy podczas **trenowania** – muszą nauczyć się odwzorowywać cały proces, co bywa bardziej wymagające niż zwykłe uczenie next-token. W praktyce szkolenie dużego modelu dyfuzyjnego (jak LLaDA 8B) pochłania ogromne zasoby: np. raportowano użycie **2.3 bln** tokenów treningowych i **130 tys. godzin GPU H800** do pretrenowania LLaDA 8B ([Paper Review: Large Language Diffusion Models – Andrey Lukyanenko](https://andlukyane.com/blog/paper-review-llada#:~:text=LLaDA%20is%20pre,13%20million%20H800%20GPU%20hours)). To porównywalne lub większe nakłady niż przy trenowaniu autoregresywnego LLM o podobnej skali. 

Jeśli chodzi o **pamięć podczas generacji**, model dyfuzyjny może potrzebować przechowywać i manipulować całymi sekwencjami na każdym kroku, co bywa wyzwaniem dla bardzo długich kontekstów. Jednak modele AR również muszą przechowywać rosnące sekwencje w buforze kontekstu, więc różnica nie jest drastyczna – chyba że $T$ jest znaczny. W najnowszych pracach dąży się do ograniczenia wymaganej liczby kroków dyfuzji. Na przykład, w modelach obrazowych opracowano techniki redukujące $T$ z setek do zaledwie kilkunastu (poprzez np. modele konsystencji). Podobne wysiłki trwają dla języka.

Podsumowując, **efektywność obliczeniowa** przechyla się na korzyść dyfuzji w kontekście *inference* (generowania) – zwłaszcza przy dłuższych wyjściach – dzięki większej równoległości i mniejszej zależności czasu wykonania od długości sekwencji. Natomiast w kontekście *trenowania* i złożoności implementacyjnej podejście dyfuzyjne jest bardziej skomplikowane i kosztowne, choć mieszczące się w skali obecnych projektów LLM. 

### 3.3 Zastosowania praktyczne

Oba podejścia – autoregresywne i dyfuzyjne – mają swoje obszary, w których sprawdzają się najlepiej, oraz potencjalne zastosowania, które lepiej adresuje jedno niż drugie.

**Modele autoregresywne** są dojrzałą technologią i obecnie stanowią trzon większości wdrożeń komercyjnych: od systemów dialogowych (asystenci pokroju ChatGPT, Claude) przez tłumaczenia maszynowe, streszczanie tekstów, aż po generatory kodu (Github Copilot, Amazon CodeWhisperer). Ich atutem jest **elastyczność długości** – mogą generować ciąg znaków dowolnej długości, po prostu kontynuując proces dopóki nie zostanie wygenerowany token końca sekwencji (EOS) lub nie spełnią innego warunku stopu. Ta właściwość jest bardzo wygodna np. w czatach, gdzie model może produkować odpowiedź dopóki jest to potrzebne. Ponadto, modele AR łatwo dostosować do **streamowania odpowiedzi** w czasie rzeczywistym – mogą zacząć zwracać użytkownikowi pierwsze tokeny od razu po ich wygenerowaniu, zanim ukończą całą odpowiedź. To poprawia interaktywność (użytkownik widzi “piszącą” AI). Wreszcie, istnieje bogaty ekosystem technik poprawy jakości generacji AR: **sampling z temperaturą, nucleus sampling, beam search, reranking**, itp., które są dobrze zrozumiane i zaimplementowane.

**Modele dyfuzyjne** dopiero zaczynają pojawiać się w praktycznych zastosowaniach, ale mają kilka unikalnych możliwości. Jedną z nich jest **kontrolowane generowanie z uzupełnianiem kontekstu** (*in-filling*). Ponieważ model dyfuzyjny może zaczynać od “szkicu” zdania, można w tym szkicu zablokować pewne fragmenty, które mają pozostać niezmienione, a wygenerować jedynie brakujące części. Przykładowo, użytkownik może podać początek **i koniec** zdania, a model wypełni środek w sposób spójny z tym kontekstem. Autoregresywny model nie umie naturalnie wygenerować tekstu na podstawie **dwustronnego kontekstu** (chyba że stosuje się specjalne architektury non-causal, co jest rzadkie). Dyfuzja natomiast natywnie wspiera takie przypadki – wystarczy rozpocząć od sekwencji gdzie środek jest zamaskowany, a początek i koniec to zadany kontekst, i wykonać denoising. Jest to analogiczne do tzw. *image inpainting* w wizji komputerowej, lecz tutaj zastosowane do tekstu. Taka kontrola generacji pozwala np. na **łatwą edycję już wygenerowanych tekstów** – można kazać modelowi poprawić konkretny fragment zdania, zamaskowawszy go, bez ryzyka że reszta zdania ulegnie zmianie (model dostosuje się do stałego kontekstu otaczającego).

Innym obszarem jest **wielomodalność**. Ermon (Inception Labs) sugeruje, że dyfuzja umożliwi stworzenie **uniwersalnego modelu generatywnego** zdolnego obsłużyć różne modalności (tekst, obraz, dźwięk) w ramach jednej architektury ([Silicon Valley Startup Inception Labs Creates Faster LLM](https://www.pymnts.com/artificial-intelligence-2/2025/silicon-valley-startup-inception-labs-creates-faster-llm/#:~:text=Diffusion%20models%20are%20also%20easier,audio%20coming%20in%20the%20future)). Wizją jest, że model taki mógłby dzielić wiedzę między zadaniami – np. doświadczenie z generowania obrazów mogłoby pomóc w rozumieniu opisów tekstowych i odwrotnie ([Silicon Valley Startup Inception Labs Creates Faster LLM](https://www.pymnts.com/artificial-intelligence-2/2025/silicon-valley-startup-inception-labs-creates-faster-llm/#:~:text=%E2%80%9CIt%20now%20becomes%20possible%20to,%E2%80%9D)). Jest to możliwe dzięki temu, że technika dyfuzji jest stosowana we wszystkich tych dziedzinach, a wejściem modelu może być ogólny “zaszumiony sygnał” dowolnej modalności. Być może przyszłe LLM-y dyfuzyjne będą rozszerzone o możliwość dołączania do promptu np. szkicu rysunku czy próbki audio, a model wygeneruje zarówno spójny tekst, jak i obraz. Choć to na razie koncepcja, już sam fakt, że Mercury planuje łączyć generowanie kodu, tekstu i docelowo obrazów w jednej rodzinie modeli, wskazuje na przewagę elastyczności tego podejścia ([Silicon Valley Startup Inception Labs Creates Faster LLM](https://www.pymnts.com/artificial-intelligence-2/2025/silicon-valley-startup-inception-labs-creates-faster-llm/#:~:text=Diffusion%20models%20are%20also%20easier,audio%20coming%20in%20the%20future)).

**Zastosowania wymagające globalnej koherencji:** Modele dyfuzyjne powinny górować w zadaniach, gdzie istotna jest ścisła zależność między odległymi fragmentami tekstu. Przykładem może być generowanie **wierszy, palindromów, tekstów o określonej strukturze**, gdzie np. ostatnia linijka musi nawiązywać do pierwszej. W modelu AR takie zadanie jest trudne – model generując pierwsze wersy nie “wie”, jak ma się zakończyć utwór, więc może po drodze zgubić restrykcje narzucone przez użytkownika. W modelu dyfuzyjnym łatwo np. narzucić ostatnią linijkę z góry i pozwolić modelowi dopasować do niej resztę. Nawet bez jawnego narzucania, zdolność modelu do widzenia pełnego kontekstu podczas generacji pomaga. Na przykład we wspomnianym wyżej **odwracaniu tekstu (reversal curse)** – zadaniu polegającym na dokończeniu wiersza tak, by czytany wspak dawał sensowny rezultat – model LLaDA osiągnął lepsze wyniki niż GPT-4 bazujący na AR ([Paper Review: Large Language Diffusion Models – Andrey Lukyanenko](https://andlukyane.com/blog/paper-review-llada#:~:text=LLaDA%20demonstrates%20strong%20scalability%2C%20outperforming,poem%20completion%20task%2C%20addressing%20the)). Świadczy to, że *bidirectional* charakter dyfuzji przekłada się na praktyczną poprawę w takich niestandardowych zadaniach.

**Podsumowanie zastosowań:** AR to “koń pociągowy” większości istniejących aplikacji generatywnego NLP – sprawdza się tam, gdzie wymagana jest interaktywność, strumieniowe podawanie odpowiedzi i gdzie generujemy treści w znanych formatach (rozmowa, akapit tekstu itp.). Dyfuzja otwiera jednak nowe możliwości: bardziej **inteligentnej edycji tekstu**, **uzupełniania braków**, lepszego **zachowania spójności globalnej** oraz integracji różnych modalności. W dziedzinie generowania kodu (która łączy cechy ścisłości i tekstu) również obserwuje się przewagi – Mercury Coder (dyfuzyjny model kodujący) osiąga jakość podobną do czołowych modeli, a jednocześnie pracuje szybciej ([Silicon Valley Startup Inception Labs Creates Faster LLM](https://www.pymnts.com/artificial-intelligence-2/2025/silicon-valley-startup-inception-labs-creates-faster-llm/#:~:text=A%20Silicon%20Valley%20startup%20founded,higher%20quality%20than%20existing%20models)) ([Silicon Valley Startup Inception Labs Creates Faster LLM](https://www.pymnts.com/artificial-intelligence-2/2025/silicon-valley-startup-inception-labs-creates-faster-llm/#:~:text=Inception%20Labs%20hit%20a%20record,Flash)). Podobnie w dialogach – LLaDA po dostrojeniu nadzorowanym okazała się zdolna do złożonych konwersacji wieloturowych na poziomie zbliżonym do tradycyjnych LLM ([Paper Review: Large Language Diffusion Models – Andrey Lukyanenko](https://andlukyane.com/blog/paper-review-llada#:~:text=LLaDA%20demonstrates%20strong%20scalability%2C%20outperforming,poem%20completion%20task%2C%20addressing%20the)).

### 3.4 Zalety i wady obu podejść

Poniżej zestawiono główne **zalety** i **wady** modeli autoregresywnych oraz dyfuzyjnych w formie tabelarycznej:

| **Aspekt**                  | **Modele autoregresywne (AR)**                                       | **Modele dyfuzyjne**                                              |
|---------------------------- |---------------------------------------------------------------------|-------------------------------------------------------------------|
| **Architektura generacji**  | Sekwencyjna (tokio po tokenie, **jednokierunkowa** – tylko kontekst lewy) ([Silicon Valley Startup Inception Labs Creates Faster LLM](https://www.pymnts.com/artificial-intelligence-2/2025/silicon-valley-startup-inception-labs-creates-faster-llm/#:~:text=%E2%80%9CAll%20existing%20large%20language%20models,%E2%80%9D)). | Iteracyjna (cała sekwencja generowana poprzez kolejne **denoising** pełnego kontekstu) ([What are Large Language Diffusion with mAsking (LLaDA)? Redefining Language Generation with Diffusion Models](https://learnprompting.org/blog/large-language-diffusion-models?srsltid=AfmBOoomRpUimmugFjLV0v1gQU6cEHp2e7rSpxHKHBpoCnCzmXs2eO5x#:~:text=1,part%20of%20the%20training%20data)) ([What are Large Language Diffusion with mAsking (LLaDA)? Redefining Language Generation with Diffusion Models](https://learnprompting.org/blog/large-language-diffusion-models?srsltid=AfmBOoomRpUimmugFjLV0v1gQU6cEHp2e7rSpxHKHBpoCnCzmXs2eO5x#:~:text=2,and%20better%20integration%20of%20context)). |
| **Użycie kontekstu**        | Tylko przeszłe tokeny podczas generacji (**kontekst jednokierunkowy**) ([What are Large Language Diffusion with mAsking (LLaDA)? Redefining Language Generation with Diffusion Models](https://learnprompting.org/blog/large-language-diffusion-models?srsltid=AfmBOoomRpUimmugFjLV0v1gQU6cEHp2e7rSpxHKHBpoCnCzmXs2eO5x#:~:text=,time)).      | Pełny kontekst bieżącej sekwencji na każdym kroku (**kontekst dwukierunkowy**) ([What are Large Language Diffusion with mAsking (LLaDA)? Redefining Language Generation with Diffusion Models](https://learnprompting.org/blog/large-language-diffusion-models?srsltid=AfmBOoomRpUimmugFjLV0v1gQU6cEHp2e7rSpxHKHBpoCnCzmXs2eO5x#:~:text=input%20text%20with%20a%20probability,part%20of%20the%20training%20data)) ([What are Large Language Diffusion with mAsking (LLaDA)? Redefining Language Generation with Diffusion Models](https://learnprompting.org/blog/large-language-diffusion-models?srsltid=AfmBOoomRpUimmugFjLV0v1gQU6cEHp2e7rSpxHKHBpoCnCzmXs2eO5x#:~:text=The%20combination%20of%20a%20randomized,both%20past%20and%20future%20context)). |
| **Równoległość generacji**  | Brak – **proces szeregowy** (każdy token po kolei, zależność czasowa) ([What are Large Language Diffusion with mAsking (LLaDA)? Redefining Language Generation with Diffusion Models](https://learnprompting.org/blog/large-language-diffusion-models?srsltid=AfmBOoomRpUimmugFjLV0v1gQU6cEHp2e7rSpxHKHBpoCnCzmXs2eO5x#:~:text=,time%20responses)).          | Wysoka – **równoległe** przewidywanie wielu tokenów w jednym kroku ([Silicon Valley Startup Inception Labs Creates Faster LLM](https://www.pymnts.com/artificial-intelligence-2/2025/silicon-valley-startup-inception-labs-creates-faster-llm/#:~:text=%E2%80%9CThe%20key%20advantage%20is%20that,words%2C%20in%20parallel%2C%E2%80%9D%20he%20said)); możliwość zmniejszenia kroków względem długości sekwencji. |
| **Czas odpowiedzi**         | Rośnie liniowo z długością wyjścia; potencjalnie wysoki dla długich tekstów. | Stały (zależny od ustalonej liczby iteracji); dla długich sekwencji zwykle znacznie krótszy niż AR ([Silicon Valley Startup Inception Labs Creates Faster LLM](https://www.pymnts.com/artificial-intelligence-2/2025/silicon-valley-startup-inception-labs-creates-faster-llm/#:~:text=Diffusion%20is%20faster%2C%20he%20said%2C,hours%20needed%20for%20using%20GPUs)). |
| **Trenowanie**              | Prostsze (bezpośrednia maksymalizacja log-wiarygodności, jednoetapowe przewidywanie tokenu). | Bardziej złożone (uczenie odtwarzania z maskowania, wieloetapowy proces, potrzeba zapewnienia zbieżności ELBO). |
| **Jakość modelowania**      | Bardzo dobre dopasowanie rozkładu dla dużych modeli; **niski perplexity** na danych testowych to ich domena. | Historycznie nieco niższa jakość modelowania prawdopodobieństwa (wyższy perplexity) – wymagało nowych rozwiązań, ale najnowsze modele zbliżają się do AR ([[2310.16834] Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution](https://arxiv.org/abs/2310.16834#:~:text=Entropy%20Discrete%20Diffusion%20models%20,sampling%20quality%20while%20enabling%20other)). |
| **Koherencja globalna**     | Czasem problematyczna (model nie “patrzy w przyszłość” – trudno wymusić zależności między odległymi tokenami). | Z definicji lepsza – model widzi całość generowanego tekstu podczas refinowania, co sprzyja spójności (np. dopasowaniu początków i zakończeń zdań). |
| **Korekta błędów**          | Brak możliwości po fakcie – **wygenerowany token jest ostateczny** (ew. można ręcznie przerwać i spróbować od nowa). | **Iteracyjna korekta** – model może poprawić wcześniej błędnie wstawiony token w kolejnych krokach denoisingu ([What are Large Language Diffusion with mAsking (LLaDA)? Redefining Language Generation with Diffusion Models](https://learnprompting.org/blog/large-language-diffusion-models?srsltid=AfmBOoomRpUimmugFjLV0v1gQU6cEHp2e7rSpxHKHBpoCnCzmXs2eO5x#:~:text=,hallucinations%20and%20improving%20overall%20accuracy)) ([What are Large Language Diffusion with mAsking (LLaDA)? Redefining Language Generation with Diffusion Models](https://learnprompting.org/blog/large-language-diffusion-models?srsltid=AfmBOoomRpUimmugFjLV0v1gQU6cEHp2e7rSpxHKHBpoCnCzmXs2eO5x#:~:text=,from%20detailed%20dialogue%20systems%20to)). |
| **Kontrolowanie generacji** | Ograniczone – łatwo kontrolować poprzez prompt początek tekstu, trudniej środek/koniec; brak natywnej obsługi uzupełniania luk. | Elastyczne – można zablokować dowolne fragmenty (np. dany początek i koniec) i model uzupełni **dowolne części tekstu** (inpainting tekstowy). Daje też inne możliwości kontroli (np. styl poprzez odpowiednie maskowanie/warunki). |
| **Streaming odpowiedzi**    | Naturalny – tokeny mogą być emitowane od razu gdy są generowane (użytkownik widzi stopniowo budowaną wypowiedź). | Utrudniony – model generuje całość po określonych iteracjach, więc nie można łatwo pokazać “częściowego” wyniku (chyba, że w blokach – patrz *Block Diffusion*). |
| **Dojrzałość ekosystemu**   | Bardzo wysoka – wiele bibliotek, zoptymalizowanych implementacji, technik redukcji halucynacji, itp., opartych o dekodowanie AR. | Nowe podejście – narzędzia dopiero powstają; mniejsza społeczność i wsparcie, na razie głównie prototypy badawcze i pierwsze wdrożenia. |
| **Przykładowe modele**      | GPT-3, GPT-4, PaLM, LLaMA, MPT, OPT, Jurrassic-2, Falcon, itp. (wszystkie generujące token po tokenie). | LLaDA (8B, 2025) ([Paper Review: Large Language Diffusion Models – Andrey Lukyanenko](https://andlukyane.com/blog/paper-review-llada#:~:text=LLaDA%20demonstrates%20strong%20scalability%2C%20outperforming,poem%20completion%20task%2C%20addressing%20the)), Mercury (2025) ([Silicon Valley Startup Inception Labs Creates Faster LLM](https://www.pymnts.com/artificial-intelligence-2/2025/silicon-valley-startup-inception-labs-creates-faster-llm/#:~:text=Inception%20Labs%20hit%20a%20record,Flash)), Diffusion-LM (Li et al. 2022), CDCD, Block Diffusion (2025) ([Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models](https://chatpaper.com/chatpaper/paper/120009#:~:text=Block%20diffusion%20sequentially%20generates%20blocks,KV%20caching%20and%20parallel%20sampling)) – modele w fazie badań i wczesnego rozwoju produktów. |

Należy podkreślić, że powyższa tabela odzwierciedla stan obecny (2024/2025) oraz ogólne właściwości. Dynamiczny rozwój tego pola może sprawić, że niektóre **wady modelu dyfuzyjnego zostaną zniwelowane** (np. poprawa efektywności trenowania, dopracowanie strumieniowej generacji blokowej), a być może **modele AR zyskają nowe możliwości** upodabniające je do dyfuzji (np. mechanizmy pozwalające na edycję wygenerowanego tekstu). Już teraz istnieją podejścia hybrydowe, co omówimy w kolejnej sekcji.

## 4. Przegląd literatury i najnowszych osiągnięć

### 4.1 **Mercury (Inception Labs, 2025)** – pierwsza komercyjna rodzina dyfuzyjnych LLM

Jednym z kamieni milowych w rozwoju dyfuzyjnych modeli językowych było ogłoszenie rodziny **Mercury** przez startup Inception Labs (założony m.in. przez Stefano Ermona z Stanford). Mercury jest reklamowany jako **pierwszy komercyjny LLM oparty na dyfuzji**, który osiąga wydajność i jakość porównywalną z klasycznymi modelami, przewyższając je znacznie pod względem szybkości ([Silicon Valley Startup Inception Labs Creates Faster LLM](https://www.pymnts.com/artificial-intelligence-2/2025/silicon-valley-startup-inception-labs-creates-faster-llm/#:~:text=A%20Silicon%20Valley%20startup%20founded,higher%20quality%20than%20existing%20models)) ([Silicon Valley Startup Inception Labs Creates Faster LLM](https://www.pymnts.com/artificial-intelligence-2/2025/silicon-valley-startup-inception-labs-creates-faster-llm/#:~:text=Diffusion%20is%20faster%2C%20he%20said%2C,hours%20needed%20for%20using%20GPUs)). Pierwszy model z tej rodziny, **Mercury Coder**, został zoptymalizowany pod kątem zadań programistycznych (generowanie kodu) i już w tej domenie wykazał swoją przewagę – w benchmarku Copilot Arena odnotowano **prędkość generacji ponad 1000 tokenów/s** na GPU H100, co dało Mercury pierwsze miejsce w szybkości oraz drugie miejsce w jakości generowanego kodu ([Silicon Valley Startup Inception Labs Creates Faster LLM](https://www.pymnts.com/artificial-intelligence-2/2025/silicon-valley-startup-inception-labs-creates-faster-llm/#:~:text=Inception%20Labs%20hit%20a%20record,Flash)). 

Mercury wykorzystuje technikę dyfuzji tekstowej, którą zespół Ermona rozwijał w ostatnich latach. W wywiadach podkreślano, że to podejście “fundamentalnie różni się” od istniejących LLM, które **wszystkie były autoregresywne** ([Silicon Valley Startup Inception Labs Creates Faster LLM](https://www.pymnts.com/artificial-intelligence-2/2025/silicon-valley-startup-inception-labs-creates-faster-llm/#:~:text=%E2%80%9CIt%E2%80%99s%20a%20fundamentally%20different%20approach%2C%E2%80%9D,in%20an%20interview%20with%20PYMNTS)). Zastosowana metoda generuje odpowiedź poprzez iteracyjne udoskonalanie zgadywanej odpowiedzi – *“zaczynamy od przybliżenia tego, jaka powinna być odpowiedź, a następnie sieć neuronowa ją doprecyzowuje aż do otrzymania finalnej odpowiedzi”* ([Silicon Valley Startup Inception Labs Creates Faster LLM](https://www.pymnts.com/artificial-intelligence-2/2025/silicon-valley-startup-inception-labs-creates-faster-llm/#:~:text=Inception%20Labs%20applies%20a%20technique,computer%20science%20at%20Stanford%20University)). Ermon zwraca uwagę, że *“kluczową zaletą jest zdolność sieci do modyfikowania wielu tokenów jednocześnie”*, co odpowiada temu, że **przetwarzanie odbywa się równolegle, a nie sekwencyjnie** ([Silicon Valley Startup Inception Labs Creates Faster LLM](https://www.pymnts.com/artificial-intelligence-2/2025/silicon-valley-startup-inception-labs-creates-faster-llm/#:~:text=%E2%80%9CThe%20key%20advantage%20is%20that,words%2C%20in%20parallel%2C%E2%80%9D%20he%20said)). Dzięki temu Mercury potrafi wygenerować długi fragment kodu lub tekstu **znacznie szybciej** niż tradycyjne modele, u których generacja odbywa się token po tokenie ([Silicon Valley Startup Inception Labs Creates Faster LLM](https://www.pymnts.com/artificial-intelligence-2/2025/silicon-valley-startup-inception-labs-creates-faster-llm/#:~:text=Diffusion%20is%20faster%2C%20he%20said%2C,hours%20needed%20for%20using%20GPUs)). 

Co istotne, Mercury jest modelem zamkniętym i komercyjnym, ale opartym na badaniach nagrodzonych wcześniej na ICML 2024 (praca Ermona i współautorów na temat dyfuzji dyskretnej ([Silicon Valley Startup Inception Labs Creates Faster LLM](https://www.pymnts.com/artificial-intelligence-2/2025/silicon-valley-startup-inception-labs-creates-faster-llm/#:~:text=Ermon%E2%80%99s%20team%20also%20thought%20about,had%20a%20breakthrough%2C%E2%80%9D%20he%20said)), prawdopodobnie chodzi tu o metodę *Score-Entropy Discrete Diffusion – SEDD*). Mimo że dokładne szczegóły architektury Mercury nie zostały ujawnione publicznie, wiadomo że **również bazuje on na Transformerze** przystosowanym do zadań maskowania zamiast auto-regresji. Andrej Karpathy skomentował publicznie, że *“od dawna było to oczekiwane, by LLM zaczęły wykorzystywać dyfuzję”*, sugerując że Mercury może prezentować **nowe unikalne możliwości i zachowania** w porównaniu do dotychczasowych modeli ([Silicon Valley Startup Inception Labs Creates Faster LLM](https://www.pymnts.com/artificial-intelligence-2/2025/silicon-valley-startup-inception-labs-creates-faster-llm/#:~:text=OpenAI%20Co,%E2%80%9D)).

Podsumowując, **Mercury** dowodzi, że modele dyfuzyjne wyszły już poza laboratorium – **komercyjne wdrożenie** tej technologii wskazuje na jej potencjał praktyczny. Zastosowanie w generowaniu kodu jest pierwszym krokiem (tu liczyła się bardzo prędkość generacji i niski koszt, co Mercury zapewnia ([Silicon Valley Startup Inception Labs Creates Faster LLM](https://www.pymnts.com/artificial-intelligence-2/2025/silicon-valley-startup-inception-labs-creates-faster-llm/#:~:text=Diffusion%20is%20faster%2C%20he%20said%2C,hours%20needed%20for%20using%20GPUs))), ale planowane są kolejne modele (dialogowy chatbot Mercury oraz API dla deweloperów) integrujące tę technologię szerzej ([Silicon Valley Startup Inception Labs Creates Faster LLM](https://www.pymnts.com/artificial-intelligence-2/2025/silicon-valley-startup-inception-labs-creates-faster-llm/#:~:text=Inception%20Labs%20has%20introduced%20a,chatbot%20and%20API%20for%20developers)). Inception Labs sygnalizuje też ambicje wykorzystania dyfuzji do wielomodalnego AI (tekst, obraz, dźwięk) w ramach jednej platformy ([Silicon Valley Startup Inception Labs Creates Faster LLM](https://www.pymnts.com/artificial-intelligence-2/2025/silicon-valley-startup-inception-labs-creates-faster-llm/#:~:text=Diffusion%20models%20are%20also%20easier,audio%20coming%20in%20the%20future)). Sukces Mercury’ego może zapoczątkować trend przechodzenia branży na modele dyfuzyjne tam, gdzie kluczowa jest wydajność i kontrola generacji.

### 4.2 **LLaDA (Large Language Diffusion with mAsking, 2025)** – dyfuzyjny LLM 8B dorównujący autoregresywnym

Równolegle do działań przemysłowych, intensywne prace badawcze nad dużymi modelami dyfuzyjnymi zaowocowały powstaniem **LLaDA** – modelu nazwanego od *Large Language Diffusion with mAsking*. Ciekawostką jest, że koncepcja LLaDA pojawiła się niemal równocześnie w dwóch niezależnych ośrodkach. Po pierwsze, zespół Shen Nie et al. opublikował na arXiv pracę *“Large Language Diffusion Models”* (luty 2025) przedstawiającą LLaDA i demonstrującą jego możliwości ([[2502.09992] Large Language Diffusion Models](https://arxiv.org/abs/2502.09992#:~:text=,and%2C%20after%20SFT%2C%20exhibits%20impressive)) ([Paper Review: Large Language Diffusion Models – Andrey Lukyanenko](https://andlukyane.com/blog/paper-review-llada#:~:text=LLaDA%20demonstrates%20strong%20scalability%2C%20outperforming,poem%20completion%20task%2C%20addressing%20the)). Po drugie, termin LLaDA pojawia się w kontekście grupy badaczy ze Stanford (Gu, Dao, Ermon, Rudra, Ré), co sugeruje, że tam również eksplorowano podobną ideę dyfuzji tekstowej z maskowaniem – być może synergicznie z projektem Mercury. Skupmy się jednak na opublikowanych wynikach.

Model LLaDA został **wytrenowany od podstaw** w paradygmacie dyfuzyjnym, osiągając rozmiar 8 miliardów parametrów. Jego architektura to Transformer działający bez maskowania przyczynowego (całkowicie *bidirectional*), trenowany na potężnym zbiorze 2.3 bln tokenów z losowym maskowaniem wejścia ([Paper Review: Large Language Diffusion Models – Andrey Lukyanenko](https://andlukyane.com/blog/paper-review-llada#:~:text=LLaDA%20is%20pre,13%20million%20H800%20GPU%20hours)). Trening przebiegał dwuetapowo: najpierw **pre-training** bez nadzoru (uczenie modelu przewidywania zamaskowanych tokenów w ogólnym korpusie tekstu), a następnie **nadzorowane dostrajanie (SFT)** na parach prompt-odpowiedź, analogicznie jak czyni się to dla klasycznych LLM ([What are Large Language Diffusion with mAsking (LLaDA)? Redefining Language Generation with Diffusion Models](https://learnprompting.org/blog/large-language-diffusion-models?srsltid=AfmBOoomRpUimmugFjLV0v1gQU6cEHp2e7rSpxHKHBpoCnCzmXs2eO5x#:~:text=Training%20and%20Supervised%20Fine)) ([Paper Review: Large Language Diffusion Models – Andrey Lukyanenko](https://andlukyane.com/blog/paper-review-llada#:~:text=%23%20Supervised%20Fine)). Podczas SFT stosowano trik: część sekwencji (prompt) pozostawiano jawnie, a maskowano tylko tokeny odpowiedzi – model uczył się więc generować odpowiedź na podstawie pełnego kontekstu promptu, co symuluje warunki generowania dialogu czy instrukcji ([Paper Review: Large Language Diffusion Models – Andrey Lukyanenko](https://andlukyane.com/blog/paper-review-llada#:~:text=LLaDA%20improves%20instruction,tokens%20are%20masked%20and%20predicted)). 

Wyniki uzyskane przez LLaDA 8B są niezwykle obiecujące. Już surowy model (po pretrainingu) wykazał **znakomitą skalowalność** – wraz ze wzrostem liczby parametrów i danych, jego metryki poprawiały się podobnie dobrze, co w analogicznych modelach AR ([Large Language Diffusion Models](https://ml-gsai.github.io/LLaDA-demo/#:~:text=Scalability)). W testach okazało się, że **LLaDA 8B osiąga porównywalne wyniki do LLaMA3 8B** (hipotetycznego autoregresywnego modelu tej samej wielkości) w zadaniach *in-context learning*, czyli np. potrafi rozwiązywać zadania z nowymi instrukcjami bazując tylko na podanych przykładach w kontekście ([Paper Review: Large Language Diffusion Models – Andrey Lukyanenko](https://andlukyane.com/blog/paper-review-llada#:~:text=LLaDA%20demonstrates%20strong%20scalability%2C%20outperforming,poem%20completion%20task%2C%20addressing%20the)). Po dostrojeniu SFT, LLaDA nabrała zdolności **podążania za instrukcjami** i prowadzenia rozmów – autorzy pokazali przykład wieloturwej konwersacji i złożonych poleceń, z którymi model radził sobie jak typowy chatbot. Co ważne, model **rozwiązał problem “reversal curse”**, czyli poprawnie uzupełniał wiersze wymagające palindromicznej struktury, **prześcigając nawet GPT-4o** (opublikowaną wersję GPT-4) w tym specyficznym zadaniu ([Paper Review: Large Language Diffusion Models – Andrey Lukyanenko](https://andlukyane.com/blog/paper-review-llada#:~:text=LLaDA%20demonstrates%20strong%20scalability%2C%20outperforming,poem%20completion%20task%2C%20addressing%20the)). To dowód na praktyczną wartość dyfuzyjnego podejścia w obszarach, gdzie AR ma problemy.

Od strony technicznej, praca o LLaDA wprowadza kilka usprawnień: m.in. wspomniane **losowe maskowanie z różnym natężeniem** zamiast stałego (co było w BERT) ([Paper Review: Large Language Diffusion Models – Andrey Lukyanenko](https://andlukyane.com/blog/paper-review-llada#:~:text=The%20training%20objective%20is%20an,scale%20applications)), brak maskowania przyczynowego w architekturze (ale kosztem rezygnacji z klasycznego cache w dekoderze) ([Paper Review: Large Language Diffusion Models – Andrey Lukyanenko](https://andlukyane.com/blog/paper-review-llada#:~:text=LLaDA%20uses%20a%20Transformer,dimension%20to%20balance%20parameter%20count)), a także pomysły w zakresie **ulepszenia procesu próbkowania**. Podczas generacji LLaDA można regulować liczbę kroków (np. generować szybciej kosztem jakości, lub wolniej dla lepszej odpowiedzi) ([Paper Review: Large Language Diffusion Models – Andrey Lukyanenko](https://andlukyane.com/blog/paper-review-llada#:~:text=LLaDA%20supports%20text%20generation%20and,likelihood%20evaluation)). Autorzy testowali różne **strategie remaskowania** – np. ponowne maskowanie $k\%$ najmniej pewnych tokenów po każdym kroku albo maskowanie losowego odsetka – by poprawić jakość generacji bez nadmiernego wydłużania procesu ([Paper Review: Large Language Diffusion Models – Andrey Lukyanenko](https://andlukyane.com/blog/paper-review-llada#:~:text=number%20of%20sampling%20steps%20controls,off%20between%20efficiency%20and%20quality)). Te zabiegi potwierdzają, że praktyczna efektywność dyfuzyjnego dekodowania zależy od sprytnego zbalansowania *ile* i *których* tokenów poprawiać na każdym etapie.

Reasumując, **LLaDA** to dowód konceptu, że **duży model językowy nie musi być autoregresywny, by dorównywać najlepszym**. Osiągnięcie podobnego poziomu co uznane LLM (w niektórych testach) przez model dyfuzyjny jest przełomowe – *obalono tym samym pogląd, że kluczowe zdolności LLM wynikają wyłącznie z mechanizmu autoregresji* ([[2502.09992] Large Language Diffusion Models](https://arxiv.org/abs/2502.09992#:~:text=benchmarks%2C%20LLaDA%20demonstrates%20strong%20scalability%2C,and%20codes%3A%20this%20https%20URL)). LLaDA wskazuje, że tak naprawdę **to maksymalizacja likelihoodu (nawet pośrednia, jak w dyfuzji) i skalowanie modelu są źródłem “inteligencji” LLM** ([Large Language Diffusion Models](https://ml-gsai.github.io/LLaDA-demo/#:~:text=We%20contend%20that%20the%20intelligence,distribution%20through%20maximum%20likelihood%20estimation)), a nie konkretnie autoregresywny sposób dekodowania. Ten wniosek otwiera drogę do dalszych badań nad ulepszaniem dyfuzyjnych LLM i być może projektowaniem hybryd.

### 4.3 **Block Diffusion (Arriola et al., ICLR 2025)** – łączenie autoregresji i dyfuzji dla elastycznej generacji

Ciekawym rozwinięciem tematu jest praca *“Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models”* (Arriola et al., 2025) ([[2503.09573] Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models](https://arxiv.org/abs/2503.09573#:~:text=,schedules%20to%20minimize%20the%20variance)). Autorzy zauważają, że podejścia AR i dyfuzyjne mają komplementarne zalety i wady: dyfuzja oferuje równoległość i kontrolę, ale dotychczasowe modele dyfuzyjne miały problemy z oceną likelihoodu i typowo wymagały założonej stałej długości sekwencji ([[2503.09573] Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models](https://arxiv.org/abs/2503.09573#:~:text=,recipe%20for%20building%20effective%20block)). Natomiast AR radzą sobie z dowolną długością (generują *do* symbolu końca) i świetnie optymalizują wiarygodność danych, kosztem sekwencyjności. Aby połączyć mocne strony obu podejść, wprowadzono **block diffusion**, czyli generowanie tekstu **blokami** tokenów.

W modelu Block Diffusion sekwencja dzielona jest na bloki (segmenty) o ustalonej długości, np. po 5 tokenów. Generacja przebiega **sekwencyjnie na poziomie bloków**, ale **wewnątrz każdego bloku odbywa się dyfuzja**. Oznacza to, że model najpierw generuje (równolegle) pierwszy blok tokenów wykorzystując dyfuzyjny dekoder, następnie używając tych wygenerowanych tokenów jako kontekstu przechodzi do generacji drugiego bloku, i tak dalej ([Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models](https://chatpaper.com/chatpaper/paper/120009#:~:text=Block%20diffusion%20sequentially%20generates%20blocks,KV%20caching%20and%20parallel%20sampling)). Tym sposobem, **długość całej sekwencji nie musi być z góry znana ani stała** – model może generować dowolnie wiele bloków jeden po drugim, dopóki nie wygeneruje bloku zawierającego token EOS (lub spełniony zostanie inny warunek stopu). Jest to zatem podejście **pół-autoregresywne (semi-AR)**: na wysokim poziomie (między blokami) zachowana jest kolejność jak w AR, ale **w obrębie bloku** model czerpie korzyści z dyfuzyjnego równoległego generowania wielu tokenów jednocześnie.

Block Diffusion wprowadza też optymalizacje, takie jak **wykorzystanie keszowania KV** dla wcześniej wygenerowanych bloków (podobnie jak w AR, nie przelicza się w kółko atencji dla kontekstu z poprzednich bloków) oraz **ulepszone harmonogramy dodawania szumu** dostosowane do danych tekstowych, by zmniejszyć wariancję gradientu podczas treningu ([[2503.09573] Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models](https://arxiv.org/abs/2503.09573#:~:text=flexible,project%20page%3A%20this%20https%20URL)). Wyniki pokazują, że model block diffusion osiąga **najlepsze jak dotąd wyniki spośród modeli dyfuzyjnych na zadaniu modelowania języka** (perplexity na korpusach), zbliżając się do modelu AR, a jednocześnie **zachowuje elastyczność długości** i pewien stopień równoległości generacji ([[2503.09573] Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models](https://arxiv.org/abs/2503.09573#:~:text=,schedules%20to%20minimize%20the%20variance)). Innymi słowy, udało się **zniwelować ograniczenie stałej długości**, które wcześniej dotyczyło dyfuzji (np. LLaDA trenowana była na sekwencjach do 4096 tokenów i radziła sobie ze zmienną długością tylko dzięki specjalnemu przygotowaniu danych w 1% przypadków ([Paper Review: Large Language Diffusion Models – Andrey Lukyanenko](https://andlukyane.com/blog/paper-review-llada#:~:text=tokens%20and%20used%200,H800%20GPU%20hours))).

Block Diffusion to ciekawy kompromis – możemy go traktować jako most między dwoma światami. Daje możliwość **generowania strumieniowego** (blok po bloku) – użytkownik mógłby otrzymywać zdanie wygenerowane blokami, co przypomina nieco *chunkowanie* odpowiedzi. Ponadto, nawet jeśli nie wszystkie tokeny generujemy równolegle (bo wciąż co pewną porcję musimy czekać na kolejny blok), to i tak uzyskujemy przyspieszenie – szczególnie, że rozmiar bloku może być np. kilkanaście tokenów, a wtedy redukujemy liczbę kroków około 10-krotnie względem czysto AR. Wreszcie, architektura block diffusion może okazać się **bardziej stabilna** niż pełna dyfuzja – ograniczając proces dyfuzyjny do krótkich segmentów, minimalizujemy ryzyko propagacji błędów na długich odcinkach. 

Warto dodać, że jednym z autorów tej pracy jest Volodymyr Kuleshov – współtwórca Inception Labs i Mercury – co wskazuje, że **hybrydowe podejścia są intensywnie badane** także w kontekście wdrożeń przemysłowych. Niewykluczone, że przyszłe komercyjne systemy zastosują block diffusion lub podobne techniki, aby połączyć najlepsze cechy dyfuzji i autoregresji.

### 4.4 Inne istotne prace i kierunki

Poza wyżej omówionymi, na rozwój dyfuzyjnych modeli językowych składają się również wcześniejsze prace, które utorowały drogę obecnym postępom. Warto wspomnieć o kilku z nich:

- **Diffusion-LM (Li et al., NeurIPS 2022)** – jeden z pierwszych działających modeli dyfuzyjnych dla języka. Wykorzystał on ciągły przestrzeń latentną (szum dodawany do osadzonych wektorów tokenów) i pokazał, że można uzyskać kontrolowane generowanie tekstu (np. o danej liczbie pozytywnych/negatywnych słów) lepsze niż AR. Był jednak ograniczony rozmiarem i szybkością (wymagał wielu kroków denoisingu).

- **Discrete Diffusion for sequence modeling (D3PM, 2022)** – prace teoretyczne nad dyfuzją na danych dyskretnych (jak tekst) zaproponowały pierwsze schematy maskowania/addytywnego szumu dyskretnego i pokazały na drobnych zbiorach (np. generowanie binarnych sekwencji) że metoda działa. To podbudowa teoretyczna pod LLaDA.

- **Score-based discrete modeling (SEDD, 2024)** – wspomniana wcześniej praca Lou, Meng, Ermon ([[2310.16834] Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution](https://arxiv.org/abs/2310.16834#:~:text=,beats%20existing%20language%20diffusion%20paradigms)) ([[2310.16834] Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution](https://arxiv.org/abs/2310.16834#:~:text=For%20comparable%20model%20sizes%2C%20SEDD,besides%20left%20to%20right%20prompting)), która wprowadziła *score-matching* do dyskretnych rozkładów przez nową funkcję kosztu (score-entropy). Udowodniono tam, że model dyfuzyjny może osiągnąć **perplexity porównywalne z GPT-2** na języku, a generując tekst potrafi kontrolować jego właściwości bez sztuczek typu dopasowywanie temperatury (wspomniano nawet 6–8x lepszą jakość generacji niż niekalibrowany GPT-2) ([[2310.16834] Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution](https://arxiv.org/abs/2310.16834#:~:text=%28reducing%20perplexity%20by%20%2425%24,besides%20left%20to%20right%20prompting)). To był ważny dowód, że **jakość statystyczna dyfuzji może dogonić AR**, co zachęciło do budowy większych modeli.

- **MaskGIT (2022)** – choć dotyczy obrazów, wprowadził koncepcję maskowanego generowania z jednoczesnym wypełnianiem wielu pikseli i adaptacyjnym remaskowaniem. Wiele pomysłów z MaskGIT przeniesiono potem na grunt języka.

- **Gibbs sampling LMs** – alternatywne podejścia pozwalające generować tekst przez iteracyjne poprawki przy użyciu klasycznych modeli (np. wykonując naprzemienne próbkowanie warunkowych rozkładów tokenów). To inny sposób na “dwukierunkową” generację. Nie jest to dyfuzja per se, ale spokrewnione idee.

Obecny trend w literaturze wskazuje, że **dyfuzja dla NLP staje się gorącym tematem**. Kolejne konferencje (ICLR 2025, ICML 2025) zawierają sesje poświęcone dyfuzyjnym modelom tekstowym, co jeszcze rok czy dwa lata temu prawie się nie zdarzało. To pokazuje, że **rozwój modeli językowych wkracza w nową fazę**, gdzie paradygmat autoregresywny przestaje być jedynym słusznym wyborem.

## 5. Wnioski i przyszłość dyfuzyjnych modeli językowych

Rozkwit dyfuzyjnych modeli językowych w połowie lat 20. XXI wieku sygnalizuje **poważną zmianę w podejściu do generowania języka naturalnego**. Dotychczasowe modele autoregresywne, choć niezwykle skuteczne, natrafiły na wyzwania skalowalności i kontroli generacji, które nowe podejścia próbują rozwiązać. **Modele dyfuzyjne** oferują świeże spojrzenie: generowanie przez **iteracyjne ulepszanie** umożliwia wprowadzenie **równoległości** i **bidirectionalnego kontekstu**, co przekłada się na praktyczne korzyści – od szybszego generowania długich odpowiedzi, przez mniejszą podatność na lokalne błędy, po większą elastyczność w zastosowaniach specjalnych (infilling, edycja, multi-modality).

W przeprowadzonej analizie porównawczej wykazaliśmy, że **żadne z podejść nie dominuje absolutnie** – każde ma swoje mocne i słabe strony, a o wyborze decyduje często konkretny **scenariusz użycia**. Niemniej jednak, obserwowane trendy sugerują, że modele dyfuzyjne będą z czasem zyskiwać przewagę w coraz większej liczbie zastosowań. Już teraz Mercury pokazał, iż mogą one dostarczyć **znaczących oszczędności czasowych i kosztowych** przy zachowaniu wysokiej jakości ([Silicon Valley Startup Inception Labs Creates Faster LLM](https://www.pymnts.com/artificial-intelligence-2/2025/silicon-valley-startup-inception-labs-creates-faster-llm/#:~:text=Diffusion%20is%20faster%2C%20he%20said%2C,hours%20needed%20for%20using%20GPUs)), a LLaDA dowiodła, że **zdolności “intelektualne” LLM (jak uczenie w kontekście czy przestrzeganie instrukcji) nie są zarezerwowane wyłącznie dla autoregresji** ([Paper Review: Large Language Diffusion Models – Andrey Lukyanenko](https://andlukyane.com/blog/paper-review-llada#:~:text=LLaDA%20demonstrates%20strong%20scalability%2C%20outperforming,poem%20completion%20task%2C%20addressing%20the)).

Jednym z głównych wniosków z przeglądu literatury jest to, że **ograniczenia modeli dyfuzyjnych są szybko pokonywane**. Problemy takie jak stała długość sekwencji czy nieoptymalne wartości log-likelihood zostały w znacznej mierze rozwiązane przez techniki typu *block diffusion* ([Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models](https://chatpaper.com/chatpaper/paper/120009#:~:text=Block%20diffusion%20sequentially%20generates%20blocks,KV%20caching%20and%20parallel%20sampling)) czy udoskonalone funkcje trenowania (score-matching dla dyskretnych danych) ([[2310.16834] Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution](https://arxiv.org/abs/2310.16834#:~:text=Entropy%20Discrete%20Diffusion%20models%20,besides%20left%20to%20right%20prompting)). Oznacza to, że argumenty przeciw dyfuzji słabną z każdą kolejną iteracją badań. Z kolei cechy unikalne dyfuzji – jak możliwość łatwej integracji wiedzy z różnych modalności czy wprowadzanie mechanizmów poprawczych – stają się coraz bardziej pożądane wraz ze wzrostem oczekiwań wobec LLM (np. żeby były edytowalne, bezpieczne i bardziej interaktywne).

Patrząc w przyszłość, można wskazać kilka potencjalnych **kierunków rozwoju** dyfuzyjnych modeli językowych:

- **Skalowanie do rozmiarów GPT-4 i większych:** Na razie największe dyfuzyjne LLM (ok. 8-20B parametrów) wciąż ustępują rozmiarem topowym modelom AR (GPT-4 ma szacunkowo >100B). W najbliższych latach prawdopodobnie zobaczymy dyfuzyjne modele o skali setek miliardów parametrów, być może wymagające nowych optymalizacji trenowania (np. lepsze harmonogramy, bardziej efektywne próbkowanie negatywne) by pozostać konkurencyjnymi w kosztach. Sukces takich modeli mógłby zrealizować wizję Ermona: *“przyszłość, w której wszystkie LLM będą oparte o paradygmat dyfuzji”* ([Silicon Valley Startup Inception Labs Creates Faster LLM](https://www.pymnts.com/artificial-intelligence-2/2025/silicon-valley-startup-inception-labs-creates-faster-llm/#:~:text=%E2%80%9CWe%E2%80%99re%20envisioning%20a%20future%20where,%E2%80%9D)).

- **Hybrydy i architektury adaptacyjne:** Być może najciekawsze systemy będą **łączyć autoregresję i dyfuzję**. Omówiony Block Diffusion to jedno podejście, ale możliwe są też inne: np. model mógłby działać autoregresywnie, ale z możliwością “refinement passes” po wygenerowaniu surowego wyniku (czyli najpierw szybko napisać tekst AR, potem parę kroków dyfuzji by go poprawić). Albo odwrotnie – model dyfuzyjny mógłby generować wersję szkicu, a potem dopisywać drobne szczegóły AR (choć to mniej sensowne). Architektury mogą też dynamicznie decydować, którą strategię obrać w zależności od zadania lub etapu generacji.

- **Wielomodalne dyfuzyjne AI:** Skoro dyfuzja jest wspólnym mianownikiem generowania obrazów, dźwięku i tekstu, należy spodziewać się, że doczekamy się modeli, które jednocześnie potrafią operować tymi typami danych. Być może będzie to jeden model z różnymi *encoders/decoders* modalności sprzęgniętymi wspólnym mechanizmem dyfuzyjnym. Taki model mógłby np. na wejściu przyjąć zapytanie tekstowe i szkic obrazu, a wygenerować dopasowany esej i ilustrację do niego – wszystko w ramach jednej koherentej odpowiedzi. Już dziś widzimy zalążki tego w postaci integracji Mercury z obrazami ([Silicon Valley Startup Inception Labs Creates Faster LLM](https://www.pymnts.com/artificial-intelligence-2/2025/silicon-valley-startup-inception-labs-creates-faster-llm/#:~:text=Diffusion%20models%20are%20also%20easier,audio%20coming%20in%20the%20future)).

- **Lepsze podstawy teoretyczne i metody trenowania:** Mimo postępów, wciąż brakuje pełnego zrozumienia, jak np. optymalnie projektować rozkład maskowania, ilu kroków potrzeba minimalnie do wysokiej jakości itp. Badania mogą przynieść bardziej **efektywne algorytmy** – np. trenowanie modelu dyfuzyjnego, który potrafi generować równie dobrze w 5 krokach co inny w 50 (podobnie jak *consistency models* w obrazach). To znów zmniejszyłoby lukę czasową do AR i może nawet uczyniło dyfuzję jednoetapową (marząc: model, który generuje perfekcyjną odpowiedź w jednym strzale wykorzystując pełny kontekst... to już właściwie byłby model *maskowany* jak BERT, ale generatywny – czyli spełnienie idei LLaDA w granicy $T\to 0$ kroków).

- **Zastosowania niszowe i specjalistyczne:** Można przypuszczać, że dyfuzja znajdzie szczególne zastosowanie tam, gdzie potrzebna jest **precyzja i zgodność z wzorcami**. Np. generowanie dokumentów prawnych, w których pewne frazy muszą się powtarzać identycznie – model dyfuzyjny może globalnie wymusić zgodność tych fragmentów (poprzez maskowanie i wspólny kontekst), podczas gdy AR czasem parafrazuje niepotrzebnie. Podobnie w generowaniu poezji czy kodu o ścisłej strukturze – iteracyjne dopracowywanie może przewyższyć jednorazowe strzały AR.

Na zakończenie, warto podkreślić, że rywalizacja “AR vs dyfuzja” to w gruncie rzeczy **dążenie do tego samego celu różnymi drogami**. Oba podejścia próbują uchwycić tę samą złożoną strukturę języka naturalnego. Sukces jednego nie oznacza porażki drugiego – przeciwnie, wiele mechanizmów wypracowanych w modelach AR (choćby same architektury Transformerów, techniki treningu typu RLHF, itp.) może być i już jest wykorzystywanych w modelach dyfuzyjnych. Być może w niedalekiej przyszłości przestaniemy w ogóle ostro rozróżniać te kategorie, a pojawi się ujednolicony framework generowania języka czerpiący inspiracje z obu. 

Już teraz jednak widzimy, że **modele dyfuzyjne dojrzały z etapu ciekawostki do etapu realnej alternatywy**. Jak ujął to Stefano Ermon, *“wyobrażamy sobie przyszłość, gdzie wszystkie LLM będą bazować na paradygmacie dyfuzji, co uczyni generatywne AI lepszym, tańszym, szybszym i wyższej jakości”* ([Silicon Valley Startup Inception Labs Creates Faster LLM](https://www.pymnts.com/artificial-intelligence-2/2025/silicon-valley-startup-inception-labs-creates-faster-llm/#:~:text=%E2%80%9CWe%E2%80%99re%20envisioning%20a%20future%20where,%E2%80%9D)). Jeśli obecne tempo rozwoju się utrzyma, ta prognoza może się sprawdzić – a dyfuzyjne modele językowe staną się integralną częścią kolejnej generacji systemów AI. 

**Bibliografia:**

1. Inception Labs (2025), *Mercury: The First Commercial Diffusion LLM Family*. – Ogłoszenie rodziny modeli Mercury, w tym Mercury Coder (model do generowania kodu). Doniesienia o 5-10x większej szybkości generacji niż klasyczne LLM oraz zastosowaniu dyfuzji do przyspieszenia modelowania języka ([Silicon Valley Startup Inception Labs Creates Faster LLM](https://www.pymnts.com/artificial-intelligence-2/2025/silicon-valley-startup-inception-labs-creates-faster-llm/#:~:text=Diffusion%20is%20faster%2C%20he%20said%2C,hours%20needed%20for%20using%20GPUs)) ([Silicon Valley Startup Inception Labs Creates Faster LLM](https://www.pymnts.com/artificial-intelligence-2/2025/silicon-valley-startup-inception-labs-creates-faster-llm/#:~:text=Inception%20Labs%20hit%20a%20record,Flash)).

2. Gu, A., Dao, T., Ermon, S., Rudra, A., & Ré, C. (2025), *LLaDA: Large Language Diffusion with mAsking*. – Praca badawcza (arXiv:2502.09992) przedstawiająca model LLaDA 8B. Pokazuje metodę maskowanej dyfuzji na dużą skalę, osiągi porównywalne z modelami autoregresywnymi oraz demonstrację rozwiązania zadania “reversal curse” ([Paper Review: Large Language Diffusion Models – Andrey Lukyanenko](https://andlukyane.com/blog/paper-review-llada#:~:text=LLaDA%20demonstrates%20strong%20scalability%2C%20outperforming,poem%20completion%20task%2C%20addressing%20the)). Zawiera omówienie architektury, treningu (2.3T tokenów, sekwencje 4k) oraz technik sampingu z ponownym maskowaniem ([Paper Review: Large Language Diffusion Models – Andrey Lukyanenko](https://andlukyane.com/blog/paper-review-llada#:~:text=LLaDA%20models%20distributions%20using%20a,applied%20only%20to%20masked%20tokens)) ([Paper Review: Large Language Diffusion Models – Andrey Lukyanenko](https://andlukyane.com/blog/paper-review-llada#:~:text=LLaDA%20supports%20text%20generation%20and,likelihood%20evaluation)).

3. Arriola, M. et al. (2025), *Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models*. – Artykuł (ICLR 2025 Oral, arXiv:2503.09573) proponujący generowanie blokowe jako hybrydę podejść. Model generuje tekst segmentami z użyciem dyfuzji wewnątrz segmentu, łącząc zalety obu światów ([Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models](https://chatpaper.com/chatpaper/paper/120009#:~:text=Block%20diffusion%20sequentially%20generates%20blocks,KV%20caching%20and%20parallel%20sampling)). Umożliwia zmienną długość sekwencji i osiąga najlepsze wyniki spośród dotychczasowych modeli dyfuzyjnych na zadaniach językowych ([[2503.09573] Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models](https://arxiv.org/abs/2503.09573#:~:text=,schedules%20to%20minimize%20the%20variance)).

4. Pymnts.com (2025), *Silicon Valley Startup Inception Labs Creates Faster LLM*. – Artykuł prasowy opisujący startup Inception Labs i ich podejście dyfuzyjne. Zawiera cytaty Stefano Ermona o różnicach między AR a dyfuzją (generacja równoległa wielu tokenów) ([Silicon Valley Startup Inception Labs Creates Faster LLM](https://www.pymnts.com/artificial-intelligence-2/2025/silicon-valley-startup-inception-labs-creates-faster-llm/#:~:text=%E2%80%9CThe%20key%20advantage%20is%20that,words%2C%20in%20parallel%2C%E2%80%9D%20he%20said)), dane o wydajności (1000 tokenów/s na H100) ([Silicon Valley Startup Inception Labs Creates Faster LLM](https://www.pymnts.com/artificial-intelligence-2/2025/silicon-valley-startup-inception-labs-creates-faster-llm/#:~:text=Inception%20Labs%20hit%20a%20record,Flash)) oraz plany rozwoju Mercury. 

5. Learnprompting.org (2025), *What are Large Language Diffusion with mAsking (LLaDA)?*. – Blog tłumaczący w przystępny sposób ideę LLaDA. Wyjaśnia ograniczenia modeli AR (brak bidirectional context, sekwencyjność) ([What are Large Language Diffusion with mAsking (LLaDA)? Redefining Language Generation with Diffusion Models](https://learnprompting.org/blog/large-language-diffusion-models?srsltid=AfmBOoomRpUimmugFjLV0v1gQU6cEHp2e7rSpxHKHBpoCnCzmXs2eO5x#:~:text=However%2C%20autoregressive%20models%20generate%20text,sequentially%2C%20which%20inherently)) i zalety modeli dyfuzyjnych (iterative refinement, równoległe przewidywanie) ([What are Large Language Diffusion with mAsking (LLaDA)? Redefining Language Generation with Diffusion Models](https://learnprompting.org/blog/large-language-diffusion-models?srsltid=AfmBOoomRpUimmugFjLV0v1gQU6cEHp2e7rSpxHKHBpoCnCzmXs2eO5x#:~:text=In%20the%20context%20of%20language,approach%20offers%20several%20potential%20advantages)), a także opisuje dwufazowy proces LLaDA (forward masking i reverse denoising) ([What are Large Language Diffusion with mAsking (LLaDA)? Redefining Language Generation with Diffusion Models](https://learnprompting.org/blog/large-language-diffusion-models?srsltid=AfmBOoomRpUimmugFjLV0v1gQU6cEHp2e7rSpxHKHBpoCnCzmXs2eO5x#:~:text=1,part%20of%20the%20training%20data)).

6. Andrey Lukyanenko (2025), *Paper Review: Large Language Diffusion Models*. – Szczegółowa recenzja pracy o LLaDA. Zawiera streszczenie metod i wyników: m.in. sformułowanie probabilistyczne maskowanej dyfuzji ([Paper Review: Large Language Diffusion Models – Andrey Lukyanenko](https://andlukyane.com/blog/paper-review-llada#:~:text=LLaDA%20models%20distributions%20using%20a,applied%20only%20to%20masked%20tokens)), informacje o treningu i architekturze (brak maskowania przyczynowego, sekwencje 4096, brak KV cache) ([Paper Review: Large Language Diffusion Models – Andrey Lukyanenko](https://andlukyane.com/blog/paper-review-llada#:~:text=LLaDA%20uses%20a%20Transformer,dimension%20to%20balance%20parameter%20count)), opis fine-tuning i inference (remasking strategy) ([Paper Review: Large Language Diffusion Models – Andrey Lukyanenko](https://andlukyane.com/blog/paper-review-llada#:~:text=LLaDA%20supports%20text%20generation%20and,likelihood%20evaluation)), a także podsumowanie osiągnięć LLaDA 8B ([Paper Review: Large Language Diffusion Models – Andrey Lukyanenko](https://andlukyane.com/blog/paper-review-llada#:~:text=LLaDA%20demonstrates%20strong%20scalability%2C%20outperforming,poem%20completion%20task%2C%20addressing%20the)).

7. ArXiv (2023), *Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution* (Lou, Meng, Ermon). – Praca wprowadzająca Score-Entropy dla dyfuzji dyskretnej. W abstrakcie raportuje znaczącą poprawę perplexity dyfuzyjnych LM i przewagę nad GPT-2 w kilku aspektach generacji ([[2310.16834] Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution](https://arxiv.org/abs/2310.16834#:~:text=Entropy%20Discrete%20Diffusion%20models%20,sampling%20quality%20while%20enabling%20other)), kładąc podwaliny teoretyczne pod nowsze modele.

8. NeurIPS (2022), *Diffusion-LM Improves Controllable Text Generation* (Li et al.). – Pionierska praca pokazująca, że modele dyfuzyjne (choć ciągłe) mogą zapewnić bardziej kontrolowalne generowanie tekstu. Wykorzystana jako wczesny dowód na potencjał dyfuzji poza obrazami.

9. CSDL (2025), *AI 2.0/LLM 2.0 – Using RL to Reason from Scratch & Diffusion Model Text Generator* (don-lim). – Wpis na Medium omawiający najnowsze trendy LLM 2.0, w tym LLaDA i inne modele dyfuzyjne, w kontekście pojawiających się nowych paradygmatów obok autoregresji.

(Przypisy do cytowanych źródeł zostały oznaczone w tekście numerami w nawiasach kwadratowych, zgodnie z formatem【numer†Linie】 odwołującym się do konkretnych fragmentów.)
